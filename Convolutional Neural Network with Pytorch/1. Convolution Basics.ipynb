{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7262f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage, misc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b2617",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "<h4 align=center>What is Convolution?</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e6d5b",
   "metadata": {},
   "source": [
    "Convolution is a linear operation similar to a linear equation, dot product, or matrix multiplication. Convolution has several advantages for analyzing images. Convolution preserves the relationship between elements, and it requires fewer parameters than other methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1118838",
   "metadata": {},
   "source": [
    "We can see the relationship between the different methods:\n",
    "\n",
    "$$linear \\ equation :y=wx+b$$\n",
    "$$linear\\ equation\\ with\\ multiple \\ variables \\ where \\ \\mathbf{x} \\ is \\ a \\ vector \\ \\mathbf{y}=\\mathbf{wx}+b$$\n",
    "$$ \\ matrix\\ multiplication \\ where \\ \\mathbf{X} \\ in \\ a \\ matrix \\ \\mathbf{y}=\\mathbf{wX}+\\mathbf{b} $$\n",
    "$$\\ convolution \\ where \\ \\mathbf{X} \\ and \\ \\mathbf{Y} \\ is \\ a \\ tensor \\  \\mathbf{Y}=\\mathbf{w}*\\mathbf{X}+\\mathbf{b}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19726b92",
   "metadata": {},
   "source": [
    "In convolution, the parameter <b>w</b> is called a kernel. we can perform convolution on images where we let the variable image denote the variable X and w denote the parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c676b7",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1xw.png\" width=\"500,\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1ca45",
   "metadata": {},
   "source": [
    "Create a two-dimensional convolution object by using the constructor Conv2d, the parameter <code>in_channels</code> and <code>out_channels</code> will be used for this section, and the parameter kernel_size will be three.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136780d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4bb6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bd980",
   "metadata": {},
   "source": [
    "Because the parameters in nn.Conv2d are randomly initialized and learned through training, let's give them some values manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e113985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[ 0.1483, -0.0017, -0.1635],\n",
       "                        [-0.0366, -0.1454,  0.1201],\n",
       "                        [ 0.1000, -0.1046,  0.1044]]]])),\n",
       "             ('bias', tensor([0.2016]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the randomly initialized values\n",
    "conv.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b83111f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[ 1.,  0., -1.],\n",
       "                        [ 2.,  0., -2.],\n",
       "                        [ 1.,  0., -1.]]]])),\n",
       "             ('bias', tensor([0.]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.state_dict()['weight'][0][0] = torch.tensor([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\n",
    "conv.state_dict()['bias'][0] = 0.0\n",
    "conv.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58032301",
   "metadata": {},
   "source": [
    "Create a dummy tensor to represent an image. The shape of the image is (1,1,5,5) where:\n",
    "\n",
    "(number of inputs, number of channels, number of rows, number of columns ) but set the third column to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abcda93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.],\n",
       "          [0., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.zeros(1, 1, 5, 5)\n",
    "image[0, 0, :, 2] = 1\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2051c7",
   "metadata": {},
   "source": [
    "Call the object <code>conv</code> on the tensor <code>image</code> as an input to perform the convolution and assign the result to the tensor <code>z</code>. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df968d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.],\n",
       "          [-4.,  0.,  4.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=conv(image)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728e3bb",
   "metadata": {},
   "source": [
    "The following animation illustrates the process, the kernel performs at the element-level multiplication on every element in the image in the corresponding region. The values are then added together. The kernel is then shifted and the process is repeated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814cb656",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1convltuon.gif\" width=\"500,\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b4f0e",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"ref1\"></a>\n",
    "<h4 align=center>Determining  the Size of the Output</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e9e1c",
   "metadata": {},
   "source": [
    "The size of the output is an important parameter. Here, we will assume square images. For rectangular images, the same formula can be used in for each dimension independently.  \n",
    "\n",
    "Let M be the size of the input and K be the size of the kernel. The size of the output is given by the following formula:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d16740",
   "metadata": {},
   "source": [
    "$$M_{new}=M-K+1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa86660",
   "metadata": {},
   "source": [
    "Create a kernel of size 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88d5d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 2\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=K)\n",
    "conv1.state_dict()['weight'][0][0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "conv1.state_dict()['bias'][0] = 0.0\n",
    "conv1.state_dict()\n",
    "conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82770b",
   "metadata": {},
   "source": [
    "Create an image of size 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9035d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4\n",
    "image1 = torch.ones(1, 1, M, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f33ddc7",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1kernal2.png\" width=\"500,\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d95945",
   "metadata": {},
   "source": [
    "The following equation provides the output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b91914",
   "metadata": {},
   "source": [
    "$$M_{new}=M-K+1$$\n",
    "$$M_{new}=4-2+1$$\n",
    "$$M_{new}=3$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad390fb",
   "metadata": {},
   "source": [
    "The following animation illustrates the process: The first iteration of the kernel overlay of the images produces one output. As the kernel is of size K, there are M-K  elements for the kernel to move in the horizontal direction. The same logic applies to the vertical direction.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb90f71",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1outsize.gif\" width=\"500,\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831595a",
   "metadata": {},
   "source": [
    "Perform the convolution and verify the size is correct:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d2c7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1: tensor([[[[4., 4., 4.],\n",
      "          [4., 4., 4.],\n",
      "          [4., 4., 4.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "z1=conv1(image1)\n",
    "print(\"z1:\",z1)\n",
    "print(\"shape:\",z1.shape[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaebe8b",
   "metadata": {},
   "source": [
    "<a id='ref3'></a>\n",
    "<h4 align=center>Zero Padding </h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5ea54",
   "metadata": {},
   "source": [
    "As we apply successive convolutions, the image will shrink. We can apply zero padding to keep the image at a reasonable size, which also holds information at the borders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618ee7b",
   "metadata": {},
   "source": [
    "In addition, we might not get integer values for the size of the kernel. Consider the following image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196c968a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6a98c",
   "metadata": {},
   "source": [
    "Try performing convolutions with the <code>kernel_size=2</code> and a <code>stride=3</code>. Use these values:\n",
    "\n",
    "$$M_{new}=\\dfrac{M-K}{stride}+1$$\n",
    "$$M_{new}=\\dfrac{4-2}{3}+1$$\n",
    "$$M_{new}=1.666$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a74911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z4: tensor([[[[4.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "z4: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\n",
    "conv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv4.state_dict()['bias'][0]=0.0\n",
    "conv4.state_dict()\n",
    "z4=conv4(image1)\n",
    "print(\"z4:\",z4)\n",
    "print(\"z4:\",z4.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b4d74d",
   "metadata": {},
   "source": [
    "We can add rows and columns of zeros around the image. This is called padding. In the constructor <code>Conv2d</code>, you specify the number of rows or columns of zeros that we want to add with the parameter padding. \n",
    "\n",
    "For a square image, we merely pad an extra column of zeros to the first column and the last column. Repeat the process for the rows. As a result, for a square image, the width and height is the original size plus 2 x the number of padding elements specified. We can then determine the size of the output after subsequent operations accordingly as shown in the following equation where we determine the size of an image after padding and then applying a convolutions kernel of size K.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84444c",
   "metadata": {},
   "source": [
    "$$M'=M+2 \\times padding$$\n",
    "$$M_{new}=M'-K+1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa480e5a",
   "metadata": {},
   "source": [
    "Consider the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)\n",
    "\n",
    "conv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv5.state_dict()['bias'][0]=0.0\n",
    "conv5.state_dict()\n",
    "z5=conv5(image1)\n",
    "print(\"z5:\",z5)\n",
    "print(\"z5:\",z4.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a4ff4",
   "metadata": {},
   "source": [
    "The process is summarized in the following  animation: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdf7d8",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.1zeropad.gif\" width=\"500,\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82750e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.8096, -0.7328,  0.5076, -0.0776],\n",
       "          [-1.0550, -0.9096,  0.3485,  0.2969],\n",
       "          [-1.7371, -1.1403,  0.2809, -0.2672],\n",
       "          [ 0.0183, -0.6505,  0.1311,  0.1464]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Practice\n",
    "\n",
    "# A kernel of zeros with a kernel size=3 is applied to the following image:\n",
    "Image=torch.randn((1,1,4,4))\n",
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b9b91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
    "conv.state_dict()['weight'][0][0]=torch.tensor([[0,0,0],[0,0,0],[0,0.0,0]])\n",
    "conv.state_dict()['bias'][0]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9d0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = conv(Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26251861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0.],\n",
       "          [0., 0.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
